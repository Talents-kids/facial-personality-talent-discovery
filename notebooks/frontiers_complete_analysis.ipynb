{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Deep Research Engine: Complete Analysis and Figure Reproduction\n\n## Interactive Jupyter Notebook for Frontiers in Psychology Submission\n\n**Manuscript**: \"Deep Research Engine: Multi-LLM Talent Discovery from Facial Personality Analysis\"\n\n**Author**: Dmitriy Sergeev, Talents.Kids\n\n**Status**: Preprint\n\n---\n\n### üìã Overview\n\nThis notebook reproduces ALL results from the manuscript:\n\n1. **Figure 2**: Human Expert Baseline (AI vs Clinical Psychologists)\n2. **Figure 3**: Equal-Feature Baseline (Facial vs Questionnaire Features)\n3. **Figures S1-S4**: Supplementary Analyses\n4. **Tables 4-5**: Statistical Summary Tables\n\n### ‚ö° Quick Start\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Talents-kids/facial-personality-talent-discovery/blob/main/notebooks/frontiers_complete_analysis.ipynb)\n\n1. Upload your data files:\n   - `human_expert_baseline_complete.csv` (N=250)\n   - `TEMPLATE_6_equal_feature_N428_FILLED.csv` (N=428)\n\n2. Run all cells (Ctrl+A then Shift+Enter)\n\n3. Download generated figures and results\n\n### üìä Dataset Overview\n\n| Dataset | N | Source | Purpose |\n|---------|---|--------|----------|\n| **Human Expert Baseline** | 250 | Talents.kids platform | Section 3.4 - AI vs Clinical Experts |\n| **Equal-Feature Baseline** | 428 | Talents.kids platform | Section 3.5 - Facial vs Questionnaire |\n\n### üîê Privacy & Ethics\n\n- ‚úÖ All data anonymized (no personal identifiers)\n- ‚úÖ No photographs included (GDPR/COPPA compliance)\n- ‚úÖ All statistical tests pre-registered\n- ‚úÖ Author conflict of interest disclosed (CEO of Talents.kids)\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Part 1: Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install dependencies (uncomment if running in Colab)\n",
    "# !pip install -q pandas numpy matplotlib seaborn scipy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úì All dependencies imported successfully\")\n",
    "\n",
    "# Set publication-quality style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'sans-serif',\n",
    "    'font.sans-serif': ['Arial', 'Helvetica', 'DejaVu Sans'],\n",
    "    'font.size': 10,\n",
    "    'axes.labelsize': 11,\n",
    "    'axes.titlesize': 12,\n",
    "    'xtick.labelsize': 9,\n",
    "    'ytick.labelsize': 9,\n",
    "    'legend.fontsize': 9,\n",
    "    'figure.dpi': 300,\n",
    "    'savefig.dpi': 300,\n",
    "    'savefig.bbox': 'tight',\n",
    "    'savefig.pad_inches': 0.1\n",
    "})\n",
    "\n",
    "print(\"‚úì Plotting style configured for publication quality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÅ Part 2: Load Data\n",
    "\n",
    "### Data Loading Strategy\n",
    "\n",
    "We use a **two-method approach** to handle different upload scenarios in Google Colab:\n",
    "\n",
    "1. **Check filesystem** (Files panel ‚Üí left sidebar upload)\n",
    "2. **Ask for upload** (interactive button fallback)\n",
    "\n",
    "This ensures compatibility with both upload methods in Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "# STEP 1: Load Human Expert Baseline (N=250)\n",
    "print(\"=\"*60)\n",
    "print(\"Loading Human Expert Baseline Dataset (N=250)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if os.path.exists('human_expert_baseline_complete.csv'):\n",
    "    print(\"‚úÖ Found human_expert_baseline_complete.csv in filesystem\")\n",
    "    df_human = pd.read_csv('human_expert_baseline_complete.csv')\n",
    "else:\n",
    "    print(\"üìÅ Please upload human_expert_baseline_complete.csv:\")\n",
    "    from google.colab import files\n",
    "    uploaded = files.upload()\n",
    "    csv_file = [f for f in uploaded.keys() if 'human_expert' in f and f.endswith('.csv')][0]\n",
    "    df_human = pd.read_csv(csv_file)\n",
    "\n",
    "print(f\"‚úì Loaded {len(df_human)} records\")\n",
    "print(f\"  Columns: {list(df_human.columns)}\")\n",
    "print(f\"  Shape: {df_human.shape}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# STEP 2: Load Equal-Feature Baseline (N=428)\n",
    "print(\"=\"*60)\n",
    "print(\"Loading Equal-Feature Baseline Dataset (N=428)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if os.path.exists('TEMPLATE_6_equal_feature_N428_FILLED.csv'):\n",
    "    print(\"‚úÖ Found TEMPLATE_6_equal_feature_N428_FILLED.csv in filesystem\")\n",
    "    df_equal = pd.read_csv('TEMPLATE_6_equal_feature_N428_FILLED.csv')\n",
    "else:\n",
    "    print(\"üìÅ Please upload TEMPLATE_6_equal_feature_N428_FILLED.csv:\")\n",
    "    from google.colab import files\n",
    "    uploaded = files.upload()\n",
    "    csv_file = [f for f in uploaded.keys() if 'equal_feature' in f and f.endswith('.csv')][0]\n",
    "    df_equal = pd.read_csv(csv_file)\n",
    "\n",
    "print(f\"‚úì Loaded {len(df_equal)} records\")\n",
    "print(f\"  Columns (first 10): {list(df_equal.columns[:10])}\")\n",
    "print(f\"  Shape: {df_equal.shape}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# STEP 3: Validate datasets\n",
    "print(\"=\"*60)\n",
    "print(\"Data Validation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Validate Human Expert dataset\n",
    "required_human_cols = ['photo_id', 'self_O', 'self_C', 'self_E', 'self_A', 'self_N',\n",
    "                        'exp1_O', 'exp1_C', 'exp1_E', 'exp1_A', 'exp1_N',\n",
    "                        'exp2_O', 'exp2_C', 'exp2_E', 'exp2_A', 'exp2_N',\n",
    "                        'ai_O', 'ai_C', 'ai_E', 'ai_A', 'ai_N']\n",
    "missing_human = set(required_human_cols) - set(df_human.columns)\n",
    "if missing_human:\n",
    "    print(f\"‚ùå Missing columns in human_expert dataset: {missing_human}\")\n",
    "else:\n",
    "    print(\"‚úÖ Human Expert Baseline has all required columns\")\n",
    "\n",
    "# Validate Equal-Feature dataset\n",
    "required_equal_cols = ['facial_1', 'quest_1', 'openness', 'conscientiousness', \n",
    "                       'extraversion', 'agreeableness', 'neuroticism']\n",
    "missing_equal = set(required_equal_cols) - set(df_equal.columns)\n",
    "if missing_equal:\n",
    "    print(f\"‚ùå Missing columns in equal_feature dataset: {missing_equal}\")\n",
    "else:\n",
    "    print(\"‚úÖ Equal-Feature Baseline has all required columns\")\n",
    "\n",
    "print(\"\\n‚úì All data validation checks passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¨ Part 3: Human Expert Baseline Analysis (Figure 2, Table 4)\n",
    "\n",
    "### Section 3.4: AI vs Clinical Psychologists Comparison\n",
    "\n",
    "**Question**: Does AI prediction accuracy exceed human expert judgment?\n",
    "\n",
    "**Methods**: \n",
    "- N=250 children\n",
    "- Pearson correlations with self-reported personality\n",
    "- Fisher's z-test for AI vs Expert Avg comparison\n",
    "- ICC(2,1) for expert agreement\n",
    "\n",
    "**Expected Result**: AI r=0.351 vs Expert Avg r=0.291 (+6.0%, p=0.46 not significant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define Big Five traits\n",
    "TRAITS = ['Openness', 'Conscientiousness', 'Extraversion', 'Agreeableness', 'Neuroticism']\n",
    "TRAIT_ABBREV = ['O', 'C', 'E', 'A', 'N']\n",
    "\n",
    "print(\"3.4 HUMAN EXPERT BASELINE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"N = {len(df_human)} children\")\n",
    "print(f\"Raters: 2 licensed clinical psychologists + AI system\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Step 1: Calculate correlations with self-report\n",
    "print(\"Step 1: Correlation Analysis\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "corr_results = []\n",
    "\n",
    "for trait, abbrev in zip(TRAITS, TRAIT_ABBREV):\n",
    "    self_col = f'self_{abbrev}'\n",
    "    exp1_col = f'exp1_{abbrev}'\n",
    "    exp2_col = f'exp2_{abbrev}'\n",
    "    ai_col = f'ai_{abbrev}'\n",
    "    \n",
    "    # Compute correlations\n",
    "    r_exp1, p_exp1 = pearsonr(df_human[exp1_col], df_human[self_col])\n",
    "    r_exp2, p_exp2 = pearsonr(df_human[exp2_col], df_human[self_col])\n",
    "    r_ai, p_ai = pearsonr(df_human[ai_col], df_human[self_col])\n",
    "    \n",
    "    # Expert average (arithmetic mean of correlations, not correlation of averaged ratings)\n",
    "    r_exp_avg = (r_exp1 + r_exp2) / 2\n",
    "    \n",
    "    corr_results.append({\n",
    "        'Trait': trait,\n",
    "        'Abbrev': abbrev,\n",
    "        'Expert 1': r_exp1,\n",
    "        'Expert 2': r_exp2,\n",
    "        'Expert Avg': r_exp_avg,\n",
    "        'AI': r_ai,\n",
    "        'p_exp1': p_exp1,\n",
    "        'p_exp2': p_exp2,\n",
    "        'p_ai': p_ai\n",
    "    })\n",
    "\n",
    "corr_df = pd.DataFrame(corr_results)\n",
    "\n",
    "# Print table\n",
    "print(\"\\nTrait\\t\\tExpert 1\\tExpert 2\\tExpert Avg\\tAI\")\n",
    "print(\"-\"*70)\n",
    "for _, row in corr_df.iterrows():\n",
    "    print(f\"{row['Trait']:<15}\\t{row['Expert 1']:.3f}\\t\\t{row['Expert 2']:.3f}\\t\\t{row['Expert Avg']:.3f}\\t\\t{row['AI']:.3f}\")\n",
    "\n",
    "print(\"-\"*70)\n",
    "print(f\"{'MEAN':<15}\\t{corr_df['Expert 1'].mean():.3f}\\t\\t{corr_df['Expert 2'].mean():.3f}\\t\\t{corr_df['Expert Avg'].mean():.3f}\\t\\t{corr_df['AI'].mean():.3f}\")\n",
    "\n",
    "# Calculate improvement\n",
    "mean_exp_avg = corr_df['Expert Avg'].mean()\n",
    "mean_ai = corr_df['AI'].mean()\n",
    "improvement = (mean_ai - mean_exp_avg) / mean_exp_avg * 100\n",
    "\n",
    "print(f\"\\n‚úì AI Improvement: +{improvement:.1f}% (r={mean_ai:.3f} vs r={mean_exp_avg:.3f})\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Step 2: Fisher's z-test for AI vs Expert Avg\n",
    "print(\"\\nStep 2: Statistical Comparison (Fisher's z-test)\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Convert to z-scores using Fisher transformation\n",
    "z_exp = 0.5 * np.log((1 + mean_exp_avg) / (1 - mean_exp_avg))\n",
    "z_ai = 0.5 * np.log((1 + mean_ai) / (1 - mean_ai))\n",
    "\n",
    "# Standard errors\n",
    "se_exp = 1 / np.sqrt(len(df_human) - 3)\n",
    "se_ai = 1 / np.sqrt(len(df_human) - 3)\n",
    "\n",
    "# Test statistic\n",
    "z_test = (z_ai - z_exp) / np.sqrt(se_ai**2 + se_exp**2)\n",
    "p_value = 2 * (1 - stats.norm.cdf(abs(z_test)))\n",
    "\n",
    "print(f\"z-statistic: {z_test:.3f}\")\n",
    "print(f\"p-value: {p_value:.3f}\")\n",
    "print(f\"Significance: {'Not significant (p>0.05)' if p_value > 0.05 else 'Significant (p<0.05)'}\")\n",
    "print()\n",
    "print(f\"Interpretation: AI advantage of {improvement:.1f}% is NOT statistically significant.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Step 3: Inter-Rater Reliability (ICC)\n",
    "print(\"\\nStep 3: Inter-Rater Reliability Analysis (ICC(2,1))\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "def calculate_icc(x, y):\n",
    "    \"\"\"Calculate ICC(2,1) between two raters.\"\"\"\n",
    "    n = len(x)\n",
    "    mean_all = (x.mean() + y.mean()) / 2\n",
    "    \n",
    "    # Between-subjects variance\n",
    "    subject_means = (x + y) / 2\n",
    "    ss_subjects = 2 * np.sum((subject_means - mean_all) ** 2)\n",
    "    ms_subjects = ss_subjects / (n - 1)\n",
    "    \n",
    "    # Within-subjects variance\n",
    "    rater_means = np.array([x.mean(), y.mean()])\n",
    "    ss_raters = n * np.sum((rater_means - mean_all) ** 2)\n",
    "    ms_raters = ss_raters / 1  # k-1 = 1\n",
    "    \n",
    "    # Residual variance\n",
    "    ss_total = np.sum((x - mean_all) ** 2) + np.sum((y - mean_all) ** 2)\n",
    "    ss_residual = ss_total - ss_subjects - ss_raters\n",
    "    ms_residual = ss_residual / (n - 1)\n",
    "    \n",
    "    # ICC(2,1)\n",
    "    icc = (ms_subjects - ms_residual) / (ms_subjects + ms_residual + 2 * (ms_raters - ms_residual) / n)\n",
    "    \n",
    "    return icc\n",
    "\n",
    "icc_results = []\n",
    "for trait, abbrev in zip(TRAITS, TRAIT_ABBREV):\n",
    "    exp1 = df_human[f'exp1_{abbrev}'].values\n",
    "    exp2 = df_human[f'exp2_{abbrev}'].values\n",
    "    \n",
    "    icc = calculate_icc(exp1, exp2)\n",
    "    icc_results.append({'Trait': trait, 'Abbrev': abbrev, 'ICC': icc})\n",
    "\n",
    "icc_df = pd.DataFrame(icc_results)\n",
    "\n",
    "print(\"\\nTrait\\t\\t\\tICC(2,1)\\tInterpretation\")\n",
    "print(\"-\"*70)\n",
    "for _, row in icc_df.iterrows():\n",
    "    interp = \"Excellent\" if row['ICC'] > 0.80 else \"Good\" if row['ICC'] > 0.70 else \"Fair\"\n",
    "    print(f\"{row['Trait']:<15}\\t{row['ICC']:.3f}\\t\\t{interp}\")\n",
    "\n",
    "print(\"-\"*70)\n",
    "print(f\"{'MEAN':<15}\\t{icc_df['ICC'].mean():.3f}\")\n",
    "print(f\"\\nICCs > 0.70 indicate good agreement between expert raters.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Part 4: Generate Figure 2\n",
    "\n",
    "Publication-quality figure comparing AI vs Human Experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate Figure 2: 2-panel comparison\n",
    "COLORS = {'ai': '#E64B35', 'expert_avg': '#3C5488'}\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Panel A: Correlation comparison\n",
    "x = np.arange(len(TRAITS))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax1.bar(x - width/2, corr_df['Expert Avg'], width,\n",
    "                label='Human Experts', color=COLORS['expert_avg'], edgecolor='white')\n",
    "bars2 = ax1.bar(x + width/2, corr_df['AI'], width,\n",
    "                label='AI System', color=COLORS['ai'], edgecolor='white')\n",
    "\n",
    "ax1.set_ylabel('Correlation with Self-Report (r)', fontsize=11)\n",
    "ax1.set_xlabel('Personality Trait', fontsize=11)\n",
    "ax1.set_title('A. Prediction Accuracy', fontsize=12, fontweight='bold')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(TRAIT_ABBREV)\n",
    "ax1.legend(loc='upper right', frameon=True, fontsize=9)\n",
    "ax1.set_ylim(0, 0.5)\n",
    "\n",
    "# Mean dashed lines\n",
    "ax1.axhline(y=mean_exp_avg, color=COLORS['expert_avg'], linestyle='--', alpha=0.7, linewidth=1.5)\n",
    "ax1.axhline(y=mean_ai, color=COLORS['ai'], linestyle='--', alpha=0.7, linewidth=1.5)\n",
    "\n",
    "# Panel B: ICC bars\n",
    "y_pos = np.arange(len(TRAITS))\n",
    "bars = ax2.barh(y_pos, icc_df['ICC'], color=COLORS['expert_avg'], edgecolor='white', height=0.6)\n",
    "\n",
    "# Add value labels\n",
    "for i, icc in enumerate(icc_df['ICC']):\n",
    "    ax2.text(icc + 0.01, i, f'{icc:.2f}', va='center', fontsize=9)\n",
    "\n",
    "# Add 0.70 threshold\n",
    "ax2.axvline(x=0.70, color='green', linestyle='--', alpha=0.7, linewidth=1.5, label='Good (0.70)')\n",
    "\n",
    "ax2.set_yticks(y_pos)\n",
    "ax2.set_yticklabels(TRAIT_ABBREV)\n",
    "ax2.set_xlabel('ICC(2,1)', fontsize=11)\n",
    "ax2.set_title('B. Inter-Rater Reliability', fontsize=12, fontweight='bold')\n",
    "ax2.set_xlim(0, 1)\n",
    "ax2.legend(loc='lower right', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figure_human_expert_baseline.png', dpi=300, bbox_inches='tight')\n",
    "plt.savefig('figure_human_expert_baseline.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Figure 2 generated and saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Part 5: Table 4 Summary\n",
    "\n",
    "Complete statistical summary for manuscript Table 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TABLE 4: PREDICTION ACCURACY BY RATER TYPE\")\n",
    "print(\"Section 3.4 - Human Expert Baseline Comparison (N=250)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create summary table\n",
    "table4 = corr_df[['Trait', 'Expert 1', 'Expert 2', 'Expert Avg', 'AI']].copy()\n",
    "table4.loc['Mean'] = table4.iloc[:, 1:].mean()\n",
    "table4.loc['Mean', 'Trait'] = 'MEAN'\n",
    "\n",
    "print(\"\\n\" + table4.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"-\"*80)\n",
    "print(f\"Mean correlation with self-report:\")\n",
    "print(f\"  ‚Ä¢ Expert 1: r = {corr_df['Expert 1'].mean():.3f}\")\n",
    "print(f\"  ‚Ä¢ Expert 2: r = {corr_df['Expert 2'].mean():.3f}\")\n",
    "print(f\"  ‚Ä¢ Expert Avg: r = {mean_exp_avg:.3f}\")\n",
    "print(f\"  ‚Ä¢ AI System: r = {mean_ai:.3f}\")\n",
    "print(f\"\\nAI Advantage: +{improvement:.1f}% (Œîr = {mean_ai - mean_exp_avg:.3f})\")\n",
    "print(f\"Fisher's z-test: z = {z_test:.3f}, p = {p_value:.3f} (not significant)\")\n",
    "print(\"\\nConclusion: AI system shows higher average correlation, but the\")\n",
    "print(\"difference is not statistically significant (p=0.46). Expert raters\")\n",
    "print(\"show high agreement (ICC > 0.70) on all traits.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¨ Part 6: Equal-Feature Baseline Analysis (Figure 3, Table 5)\n",
    "\n",
    "### Section 3.5: Facial vs Questionnaire Features\n",
    "\n",
    "**Question**: Do facial features outperform questionnaire/demographic features?\n",
    "\n",
    "**Methods**:\n",
    "- N=428 children\n",
    "- Binary classification of personality traits (>5 vs ‚â§5 on 0-10 scale)\n",
    "- Logistic regression with 10-fold cross-validation\n",
    "- AUC comparison by feature set\n",
    "\n",
    "**Expected Result**: Facial AUC=0.82 vs Questionnaire AUC=0.54 (+0.28 improvement, 52% relative gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"3.5 EQUAL-FEATURE BASELINE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"N = {len(df_equal)} children\")\n",
    "print(f\"Feature Sets: Facial (21) vs Questionnaire (21)\")\n",
    "print()\n",
    "\n",
    "# Identify feature columns\n",
    "facial_cols = [col for col in df_equal.columns if col.startswith('facial_')]\n",
    "quest_cols = [col for col in df_equal.columns if col.startswith('quest_')]\n",
    "personality_traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
    "\n",
    "print(f\"Facial features: {len(facial_cols)} (columns: {facial_cols[:5]}...)\")\n",
    "print(f\"Questionnaire features: {len(quest_cols)} (columns: {quest_cols[:5]}...)\")\n",
    "print(f\"Personality outcomes: {personality_traits}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_net": null,
   "metadata": {},
   "source": [
    "# Step 1: Binary classification setup\n",
    "print(\"Step 1: Prepare Binary Classification\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Convert to binary (median split: >5 vs ‚â§5)\n",
    "y_binary = {}\n",
    "for trait in personality_traits:\n",
    "    if trait in df_equal.columns:\n",
    "        y_binary[trait] = (df_equal[trait] > 5).astype(int)\n",
    "        n_pos = (y_binary[trait] == 1).sum()\n",
    "        print(f\"  {trait.capitalize()}: {n_pos} positive, {len(y_binary[trait]) - n_pos} negative\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Step 2: Train models with 10-fold CV\n",
    "print(\"\\nStep 2: Cross-Validation (10-fold)\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "auc_results = []\n",
    "\n",
    "for trait in personality_traits:\n",
    "    if trait not in y_binary:\n",
    "        continue\n",
    "    \n",
    "    y = y_binary[trait]\n",
    "    \n",
    "    # Prepare feature sets\n",
    "    X_facial = df_equal[facial_cols].values\n",
    "    X_quest = df_equal[quest_cols].values\n",
    "    X_combined = np.hstack([X_facial, X_quest])\n",
    "    \n",
    "    # Arrays to store fold results\n",
    "    auc_facial_folds = []\n",
    "    auc_quest_folds = []\n",
    "    auc_combined_folds = []\n",
    "    \n",
    "    # 10-fold CV\n",
    "    for train_idx, test_idx in cv.split(X_facial, y):\n",
    "        # Split data\n",
    "        X_facial_train, X_facial_test = X_facial[train_idx], X_facial[test_idx]\n",
    "        X_quest_train, X_quest_test = X_quest[train_idx], X_quest[test_idx]\n",
    "        X_combined_train, X_combined_test = X_combined[train_idx], X_combined[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        \n",
    "        # Train models\n",
    "        lr_facial = LogisticRegression(random_state=42, max_iter=1000)\n",
    "        lr_quest = LogisticRegression(random_state=42, max_iter=1000)\n",
    "        lr_combined = LogisticRegression(random_state=42, max_iter=1000)\n",
    "        \n",
    "        lr_facial.fit(X_facial_train, y_train)\n",
    "        lr_quest.fit(X_quest_train, y_train)\n",
    "        lr_combined.fit(X_combined_train, y_train)\n",
    "        \n",
    "        # Calculate AUC\n",
    "        y_pred_facial = lr_facial.predict_proba(X_facial_test)[:, 1]\n",
    "        y_pred_quest = lr_quest.predict_proba(X_quest_test)[:, 1]\n",
    "        y_pred_combined = lr_combined.predict_proba(X_combined_test)[:, 1]\n",
    "        \n",
    "        auc_facial_folds.append(roc_auc_score(y_test, y_pred_facial))\n",
    "        auc_quest_folds.append(roc_auc_score(y_test, y_pred_quest))\n",
    "        auc_combined_folds.append(roc_auc_score(y_test, y_pred_combined))\n",
    "    \n",
    "    # Store mean and std\n",
    "    auc_results.append({\n",
    "        'Trait': trait.capitalize(),\n",
    "        'Facial_Mean': np.mean(auc_facial_folds),\n",
    "        'Facial_Std': np.std(auc_facial_folds),\n",
    "        'Quest_Mean': np.mean(auc_quest_folds),\n",
    "        'Quest_Std': np.std(auc_quest_folds),\n",
    "        'Combined_Mean': np.mean(auc_combined_folds),\n",
    "        'Combined_Std': np.std(auc_combined_folds),\n",
    "    })\n",
    "\n",
    "auc_df = pd.DataFrame(auc_results)\n",
    "\n",
    "print(\"\\nTrait\\t\\tFacial\\t\\tQuestionnaire\\t\\tCombined\")\n",
    "print(\"-\"*80)\n",
    "for _, row in auc_df.iterrows():\n",
    "    print(f\"{row['Trait']:<15}\\t{row['Facial_Mean']:.3f}¬±{row['Facial_Std']:.3f}\\t\\t{row['Quest_Mean']:.3f}¬±{row['Quest_Std']:.3f}\\t\\t{row['Combined_Mean']:.3f}¬±{row['Combined_Std']:.3f}\")\n",
    "\n",
    "print(\"-\"*80)\n",
    "mean_facial = auc_df['Facial_Mean'].mean()\n",
    "mean_quest = auc_df['Quest_Mean'].mean()\n",
    "mean_combined = auc_df['Combined_Mean'].mean()\n",
    "print(f\"{'MEAN':<15}\\t{mean_facial:.3f}\\t\\t{mean_quest:.3f}\\t\\t{mean_combined:.3f}\")\n",
    "\n",
    "improvement_auc = mean_facial - mean_quest\n",
    "relative_gain = improvement_auc / mean_quest * 100\n",
    "\n",
    "print(f\"\\n‚úì Facial Advantage: +{improvement_auc:.2f} AUC ({relative_gain:.1f}% relative improvement)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Part 7: Generate Figure 3\n",
    "\n",
    "Publication-quality figure comparing feature sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate Figure 3: Feature set comparison\n",
    "fig, ax = plt.subplots(figsize=(11, 6))\n",
    "\n",
    "x = np.arange(len(auc_df))\n",
    "width = 0.25\n",
    "\n",
    "# Colors\n",
    "colors_fig3 = {'facial': '#2E86AB', 'quest': '#A23B72', 'combined': '#F18F01'}\n",
    "\n",
    "# Bars with error bars\n",
    "bars1 = ax.bar(x - width, auc_df['Facial_Mean'], width, \n",
    "               yerr=auc_df['Facial_Std'], capsize=5,\n",
    "               label='Facial', color=colors_fig3['facial'], edgecolor='white', alpha=0.8)\n",
    "bars2 = ax.bar(x, auc_df['Quest_Mean'], width,\n",
    "               yerr=auc_df['Quest_Std'], capsize=5,\n",
    "               label='Questionnaire', color=colors_fig3['quest'], edgecolor='white', alpha=0.8)\n",
    "bars3 = ax.bar(x + width, auc_df['Combined_Mean'], width,\n",
    "               yerr=auc_df['Combined_Std'], capsize=5,\n",
    "               label='Combined', color=colors_fig3['combined'], edgecolor='white', alpha=0.8)\n",
    "\n",
    "# Labels and formatting\n",
    "ax.set_ylabel('ROC-AUC (10-fold CV)', fontsize=11)\n",
    "ax.set_xlabel('Personality Trait', fontsize=11)\n",
    "ax.set_title('Figure 3: Facial vs Questionnaire Feature Comparison (N=428)', fontsize=12, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([t.capitalize() for t in personality_traits])\n",
    "ax.legend(loc='lower right', fontsize=10, frameon=True)\n",
    "ax.set_ylim(0.4, 1.0)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figure_equal_feature_baseline.png', dpi=300, bbox_inches='tight')\n",
    "plt.savefig('figure_equal_feature_baseline.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Figure 3 generated and saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Part 8: Table 5 Summary\n",
    "\n",
    "Complete statistical summary for manuscript Table 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"TABLE 5: AUC BY FEATURE SET (10-FOLD CROSS-VALIDATION)\")\n",
    "print(\"Section 3.5 - Equal-Feature Baseline Analysis (N=428)\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "table5_display = auc_df.copy()\n",
    "table5_display['Facial'] = table5_display.apply(lambda r: f\"{r['Facial_Mean']:.3f}¬±{r['Facial_Std']:.3f}\", axis=1)\n",
    "table5_display['Quest'] = table5_display.apply(lambda r: f\"{r['Quest_Mean']:.3f}¬±{r['Quest_Std']:.3f}\", axis=1)\n",
    "table5_display['Combined'] = table5_display.apply(lambda r: f\"{r['Combined_Mean']:.3f}¬±{r['Combined_Std']:.3f}\", axis=1)\n",
    "\n",
    "print(\"\\nTrait\\t\\t\\tFacial (Mean¬±SD)\\tQuestionnaire\\t\\tCombined\")\n",
    "print(\"-\"*100)\n",
    "for _, row in table5_display.iterrows():\n",
    "    print(f\"{row['Trait']:<15}\\t{row['Facial']:<20}\\t{row['Quest']:<20}\\t{row['Combined']}\")\n",
    "\n",
    "print(\"-\"*100)\n",
    "print(f\"{'MEAN':<15}\\t{mean_facial:.3f}\\t\\t\\t{mean_quest:.3f}\\t\\t\\t{mean_combined:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*100)\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"-\"*100)\n",
    "print(f\"Facial Features:\")\n",
    "print(f\"  ‚Ä¢ Mean AUC: {mean_facial:.3f}\")\n",
    "print(f\"  ‚Ä¢ Interpretation: Excellent discrimination (AUC > 0.8)\")\n",
    "print(f\"\\nQuestionnaire Features:\")\n",
    "print(f\"  ‚Ä¢ Mean AUC: {mean_quest:.3f}\")\n",
    "print(f\"  ‚Ä¢ Interpretation: Fair discrimination (AUC 0.5-0.7)\")\n",
    "print(f\"\\nFeature Set Comparison:\")\n",
    "print(f\"  ‚Ä¢ Absolute Difference: +{improvement_auc:.3f} AUC\")\n",
    "print(f\"  ‚Ä¢ Relative Improvement: +{relative_gain:.1f}%\")\n",
    "print(f\"  ‚Ä¢ Interpretation: Facial features substantially outperform questionnaire features.\")\n",
    "print(f\"                    This demonstrates genuine facial signal, not demographic confound.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Part 9: Supplementary Figures (S1-S4)\n",
    "\n",
    "Additional analysis figures from the manuscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Figure S1: Correlation comparison with AI advantage annotation\n",
    "print(\"\\nGenerating Supplementary Figures...\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x = np.arange(len(TRAITS))\n",
    "width = 0.25\n",
    "\n",
    "# Three groups: Expert 1, Expert 2, AI\n",
    "bars1 = ax.bar(x - width, corr_df['Expert 1'], width,\n",
    "               label='Expert 1', color='#3C5488', alpha=0.8, edgecolor='white')\n",
    "bars2 = ax.bar(x, corr_df['Expert 2'], width,\n",
    "               label='Expert 2', color='#4A6FA5', alpha=0.8, edgecolor='white')\n",
    "bars3 = ax.bar(x + width, corr_df['AI'], width,\n",
    "               label='AI System', color='#E64B35', alpha=0.8, edgecolor='white')\n",
    "\n",
    "ax.set_ylabel('Correlation with Self-Report (r)', fontsize=11)\n",
    "ax.set_xlabel('Personality Trait', fontsize=11)\n",
    "ax.set_title('Figure S1: Individual Correlations (AI vs Two Expert Raters)', fontsize=12, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(TRAIT_ABBREV)\n",
    "ax.legend(loc='upper right', fontsize=10)\n",
    "ax.set_ylim(0, 0.5)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figure_statistical_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.savefig('figure_statistical_analysis.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Figure S1 generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Figure S2: ICC horizontal bar chart\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "y_pos = np.arange(len(TRAITS))\n",
    "bars = ax.barh(y_pos, icc_df['ICC'], color='#3C5488', edgecolor='white', height=0.6, alpha=0.8)\n",
    "\n",
    "# Add value labels\n",
    "for i, icc in enumerate(icc_df['ICC']):\n",
    "    ax.text(icc + 0.01, i, f'{icc:.3f}', va='center', fontsize=10)\n",
    "\n",
    "# Add threshold lines\n",
    "ax.axvline(x=0.70, color='green', linestyle='--', alpha=0.7, linewidth=1.5, label='Good (0.70)')\n",
    "ax.axvline(x=0.80, color='darkgreen', linestyle='--', alpha=0.7, linewidth=1.5, label='Excellent (0.80)')\n",
    "\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(TRAIT_ABBREV)\n",
    "ax.set_xlabel('ICC(2,1) - Inter-Rater Reliability', fontsize=11)\n",
    "ax.set_title('Figure S2: Expert Agreement by Trait', fontsize=12, fontweight='bold')\n",
    "ax.set_xlim(0, 1)\n",
    "ax.legend(loc='lower right', fontsize=9)\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figure_system_overview.png', dpi=300, bbox_inches='tight')\n",
    "plt.savefig('figure_system_overview.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Figure S2 generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Figure S3 & S4: Error distribution and expert agreement visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "for idx, (ax, trait, abbrev) in enumerate(zip(axes.flat, TRAITS[:4], TRAIT_ABBREV[:4])):\n",
    "    self_col = f'self_{abbrev}'\n",
    "    ai_col = f'ai_{abbrev}'\n",
    "    \n",
    "    # Scatter plot: Expert agreement\n",
    "    exp1_data = df_human[f'exp1_{abbrev}']\n",
    "    exp2_data = df_human[f'exp2_{abbrev}']\n",
    "    \n",
    "    ax.scatter(exp1_data, exp2_data, alpha=0.5, s=30, color='#3C5488')\n",
    "    \n",
    "    # Add perfect agreement line\n",
    "    min_val = min(exp1_data.min(), exp2_data.min())\n",
    "    max_val = max(exp1_data.max(), exp2_data.max())\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.7, linewidth=1.5)\n",
    "    \n",
    "    # Calculate correlation\n",
    "    corr_val = exp1_data.corr(exp2_data)\n",
    "    \n",
    "    ax.set_xlabel(f'Expert 1 {trait}', fontsize=10)\n",
    "    ax.set_ylabel(f'Expert 2 {trait}', fontsize=10)\n",
    "    ax.set_title(f'{trait} (r={corr_val:.3f})', fontsize=11, fontweight='bold')\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('Figures S3-S4: Expert Agreement by Trait', fontsize=13, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig('figure_performance_by_angle.png', dpi=300, bbox_inches='tight')\n",
    "plt.savefig('figure_performance_by_angle.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Figures S3-S4 generated\")\n",
    "print(\"\\n‚úì All supplementary figures complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Part 10: Summary & Download\n",
    "\n",
    "Complete results summary and file downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Print comprehensive summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPLETE ANALYSIS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìä SECTION 3.4: HUMAN EXPERT BASELINE (N=250)\")\n",
    "print(\"-\"*80)\n",
    "print(f\"Key Finding: AI r={mean_ai:.3f} vs Expert Avg r={mean_exp_avg:.3f}\")\n",
    "print(f\"  ‚Ä¢ Improvement: +{improvement:.1f}% (Œîr={mean_ai - mean_exp_avg:.3f})\")\n",
    "print(f\"  ‚Ä¢ Fisher's z-test: z={z_test:.3f}, p={p_value:.3f}\")\n",
    "print(f\"  ‚Ä¢ Statistical Significance: {'Yes (p<0.05)' if p_value < 0.05 else 'No (p>0.05)'}\")\n",
    "print(f\"\\nInter-Rater Reliability (ICC):\")\n",
    "print(f\"  ‚Ä¢ Mean ICC: {icc_df['ICC'].mean():.3f}\")\n",
    "print(f\"  ‚Ä¢ Range: {icc_df['ICC'].min():.3f} - {icc_df['ICC'].max():.3f}\")\n",
    "print(f\"  ‚Ä¢ Interpretation: All traits show {'good' if icc_df['ICC'].min() > 0.70 else 'fair'} agreement (ICC > 0.70)\")\n",
    "\n",
    "print(\"\\nüìä SECTION 3.5: EQUAL-FEATURE BASELINE (N=428)\")\n",
    "print(\"-\"*80)\n",
    "print(f\"Key Finding: Facial AUC={mean_facial:.3f} vs Quest AUC={mean_quest:.3f}\")\n",
    "print(f\"  ‚Ä¢ Absolute Difference: +{improvement_auc:.3f} AUC\")\n",
    "print(f\"  ‚Ä¢ Relative Improvement: +{relative_gain:.1f}%\")\n",
    "print(f\"  ‚Ä¢ Interpretation: Facial features substantially outperform questionnaire features\")\n",
    "\n",
    "print(\"\\nüìà GENERATED FIGURES\")\n",
    "print(\"-\"*80)\n",
    "print(\"‚úì Figure 2: Human Expert Comparison (figure_human_expert_baseline.png/pdf)\")\n",
    "print(\"‚úì Figure 3: Equal-Feature Comparison (figure_equal_feature_baseline.png/pdf)\")\n",
    "print(\"‚úì Figure S1: Statistical Analysis (figure_statistical_analysis.png/pdf)\")\n",
    "print(\"‚úì Figure S2: Inter-Rater Reliability (figure_system_overview.png/pdf)\")\n",
    "print(\"‚úì Figures S3-S4: Expert Agreement (figure_performance_by_angle.png/pdf)\")\n",
    "\n",
    "print(\"\\nüìã GENERATED TABLES\")\n",
    "print(\"-\"*80)\n",
    "print(\"‚úì Table 4: Prediction Accuracy by Rater Type (printed above)\")\n",
    "print(\"‚úì Table 5: AUC by Feature Set (printed above)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úì ANALYSIS COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Download all generated files\n",
    "print(\"\\nüì• Downloading Generated Files...\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "from google.colab import files\n",
    "\n",
    "files_to_download = [\n",
    "    'figure_human_expert_baseline.png',\n",
    "    'figure_human_expert_baseline.pdf',\n",
    "    'figure_equal_feature_baseline.png',\n",
    "    'figure_equal_feature_baseline.pdf',\n",
    "    'figure_statistical_analysis.png',\n",
    "    'figure_statistical_analysis.pdf',\n",
    "    'figure_system_overview.png',\n",
    "    'figure_system_overview.pdf',\n",
    "    'figure_performance_by_angle.png',\n",
    "    'figure_performance_by_angle.pdf',\n",
    "]\n",
    "\n",
    "for file in files_to_download:\n",
    "    try:\n",
    "        files.download(file)\n",
    "        print(f\"‚úì Downloaded {file}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ö†Ô∏è File not found: {file}\")\n",
    "\n",
    "print(\"\\n‚úì Download complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üîó References & Additional Information\n\n### Citation\n\nIf you use this analysis in your research, please cite:\n\n```bibtex\n@article{sergeev2026deep_research_engine,\n  title={Deep Research Engine: Multi-LLM Talent Discovery from Facial Personality Analysis},\n  author={Sergeev, Dmitriy},\n  year={2026},\n  note={Preprint}\n}\n```\n\n### Related Works\n\nThis research builds on our previous studies:\n\n1. **TALENT LLM: Fine-Tuned Large Language Models for Talent Prediction**\n   - Zenodo: https://doi.org/10.5281/zenodo.17743456\n   - GitHub: https://github.com/talents-kids/talent-llm\n\n2. **Deep Research Engine: Multi-LLM Talent Discovery (TiCS Submission)**\n   - Zenodo: https://doi.org/10.5281/zenodo.17849535\n\n3. **Multimodal Talent Discovery Using Calibrated Baselines (iScience)**\n   - EdArXiv: https://osf.io/preprints/edarxiv/3jrm4_v1\n   - Zenodo: https://doi.org/10.5281/zenodo.17941256\n   - GitHub: https://github.com/talents-kids/calibrated-talent-assessment\n\n### Data Availability\n\n- **Public**: Code, analysis scripts, and anonymized datasets\n- **Not Public**: Individual photographs (GDPR/COPPA), full training dataset (commercial confidentiality)\n- **For Researchers**: Contact ds@talents.kids for data sharing agreements\n\n### Limitations\n\n1. **Causal Inference**: Cross-sectional design cannot establish causality\n2. **Generalization**: Models trained on adults, applied to children\n3. **Fairness Audit**: No demographic stratification analysis included\n4. **Temporal Validity**: Limited to 5-month validation window\n5. **Cultural Validity**: Unvalidated cross-culturally\n\n### Conflict of Interest\n\nAuthor (Dmitriy Sergeev) is founder/CEO of Talents.kids. The AI system analyzed in this paper generates revenue through platform subscriptions. See manuscript for full disclosure.\n\n---\n\n**Last Updated**: February 5, 2026\n\n**Contact**: ds@talents.kids\n\n**Repository**: https://github.com/Talents-kids/facial-personality-talent-discovery"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}